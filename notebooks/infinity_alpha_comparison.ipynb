{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ∞-Sector Convergence vs Operator Strength (α-Comparison)\n",
        "\n",
        "This notebook compares ∞-sector-style convergence for different values of the scaling parameter $\\alpha$.\n",
        "\n",
        "Given the pseudoinverse of the DLSFH Laplacian, $L^+$, we define\n",
        "$$ K_{\\alpha} = \\alpha \\frac{L^+}{\\lVert L^+ \\rVert_2}, \\quad \\alpha \\in (0,1), $$\n",
        "so that $\\lVert K_{\\alpha} \\rVert_2 \\leq \\alpha$.\n",
        "\n",
        "For each $\\alpha$, we form partial sums\n",
        "$$ S_N^{(\\alpha)} = \\sum_{n=0}^N K_{\\alpha}^n, $$\n",
        "use a fixed cutoff $N_{\\max}$ as an approximation to the ∞-sector limit, and track\n",
        "$$ \\mathrm{err}_2^{(\\alpha)}(N) = \\lVert S_N^{(\\alpha)} - S_{N_{\\max}}^{(\\alpha)} \\rVert_2. $$\n",
        "\n",
        "We then plot $\\mathrm{err}_2^{(\\alpha)}(N)$ on a common log-scale axis to visualize how the convergence\n",
        "depends on $\\alpha$ (i.e., on the effective spectral radius of $K_{\\alpha}$).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load pseudoinverse\n",
        "L_pinv_path = \"../data/Delta20_pinv.npy\"  # adjust if needed\n",
        "L_pinv = np.load(L_pinv_path)\n",
        "print(\"L^+ shape:\", L_pinv.shape)\n",
        "\n",
        "# Operator norm of L^+\n",
        "op_norm = np.linalg.norm(L_pinv, ord=2)\n",
        "print(\"Operator norm ||L^+||_2 =\", op_norm)\n",
        "\n",
        "# Alpha values to compare\n",
        "alphas = [0.3, 0.5, 0.7, 0.9]\n",
        "print(\"Comparing α values:\", alphas)\n",
        "\n",
        "N_max = 50\n",
        "dim = L_pinv.shape[0]\n",
        "I = np.eye(dim)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute error curves for each α\n",
        "\n",
        "For each $\\alpha$:\n",
        "- Construct $K_{\\alpha}$\n",
        "- Build $S_{N_{\\max}}^{(\\alpha)}$\n",
        "- Compute $\\mathrm{err}_2^{(\\alpha)}(N)$ for $N = 0, \\dots, N_{\\max}$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Ns = list(range(N_max + 1))\n",
        "errors_by_alpha = {}\n",
        "\n",
        "for alpha in alphas:\n",
        "    print(\"\\n[α =\", alpha, \"]\")\n",
        "    # build K_alpha\n",
        "    K = alpha * (L_pinv / op_norm)\n",
        "    print(\"Approximate ||K_α||_2 <=\", alpha)\n",
        "\n",
        "    # build S_Nmax^{(α)}\n",
        "    S_Nmax = np.zeros_like(K)\n",
        "    current = I.copy()\n",
        "    for n in range(N_max + 1):\n",
        "        S_Nmax += current\n",
        "        current = current @ K\n",
        "\n",
        "    # now generate partial sums and errors\n",
        "    errors_2 = []\n",
        "    S_N = np.zeros_like(K)\n",
        "    current = I.copy()\n",
        "\n",
        "    for N in Ns:\n",
        "        if N == 0:\n",
        "            S_N = current.copy()\n",
        "        else:\n",
        "            S_N += current\n",
        "        diff = S_N - S_Nmax\n",
        "        err2 = np.linalg.norm(diff, ord=2)\n",
        "        errors_2.append(err2)\n",
        "        current = current @ K\n",
        "\n",
        "    errors_by_alpha[alpha] = np.array(errors_2)\n",
        "    print(\"  initial error (N=0):\", errors_2[0])\n",
        "    print(\"  final error   (N=N_max):\", errors_2[-1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Log-scale comparison plot\n",
        "\n",
        "We now plot $\\mathrm{err}_2^{(\\alpha)}(N)$ for all $\\alpha$ on the same set of axes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(6, 4))\n",
        "\n",
        "for alpha in alphas:\n",
        "    ax.plot(Ns, errors_by_alpha[alpha], marker=\"o\", label=fr\"α = {alpha}\")\n",
        "\n",
        "ax.set_yscale(\"log\")\n",
        "ax.set_xlabel(\"N (partial sum index)\")\n",
        "ax.set_ylabel(r\"$||S_N^{(α)} - S_{N_{max}}^{(α)}||_2$\")\n",
        "ax.set_title(r\"∞-sector convergence vs $α$ (operator strength)\")\n",
        "ax.grid(True, which=\"both\", ls=\":\", alpha=0.6)\n",
        "ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpretation\n",
        "\n",
        "- Smaller $α$ (smaller effective spectral radius) leads to faster decay of the error curve.\n",
        "- As $α$ approaches 1, convergence becomes slower, reflecting the behavior expected from a geometric series.\n",
        "\n",
        "In the context of VID, this toy model illustrates how the strength of the underlying ∞-sector operator controls\n",
        "the rate at which the nonperturbative tower converges to a stable geometric object.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
